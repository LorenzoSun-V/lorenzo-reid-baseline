# encoding: utf-8
"""
@author:  lorenzo
@contact: baiyingpoi123@gmail.com
"""
import argparse
import os
import sys
import torch
from torch.nn.parallel import DistributedDataParallel
from pytorch_nndct import get_pruning_runner
sys.path.append('.')
from config.config import get_cfg
from playreid.data import build_reid_train_loader
from playreid.engine.engine import do_test, auto_scale_hyperparams, default_setup
from playreid.engine.lanuch import launch
from playreid.evaluation.testing import flatten_results_dict
from playreid.modeling import build_model
from playreid.utils import comm
from playreid.utils.checkpoint import Checkpointer
from playreid.utils.events import (
    CommonMetricPrinter,
    EventStorage,
    JSONWriter,
    TensorboardXWriter
)

from playreid.solver import build_lr_scheduler, build_optimizer


def default_argument_parser():
    """
    Create a parser with some common arguments used.
    Returns:
        argparse.ArgumentParser:
    """
    parser = argparse.ArgumentParser(description="Lorenzo ReID Baseline Training")
    parser.add_argument("-f", "--config-file", default="/workspace/lorenzo/ReID/lorenzo-reid-baseline/configs/Market1501/bagtricks_R50.yml", type=str, help="path to config file")
    parser.add_argument("-s", "--sparsity", type=float, default=0.5, help="sparsity ratio of model")
    parser.add_argument("--last-sparsity", type=float, default=0, help="sparsity ratio of model last time")
    parser.add_argument("-d", "--num-gpus", type=int, default=1, help="number of gpus *per machine*")
    parser.add_argument("--num-machines", type=int, default=1, help="total number of machines")
    parser.add_argument("--machine-rank", type=int, default=0, help="the rank of this machine (unique per machine)")
    port = 2 ** 15 + 2 ** 14 + hash(os.getuid() if sys.platform != "win32" else 1) % 2 ** 14
    parser.add_argument("--dist-url", default="tcp://127.0.0.1:{}".format(port))
    parser.add_argument("opts", help="Modify config options using the command-line", default=None, nargs=argparse.REMAINDER)
    return parser

    
def setup(args):
    """
    Create configs and perform basic setups.
    """
    # This function loads default configuration written in ./config/default.py .
    cfg = get_cfg()
    # Merge configs from a given yaml file.
    cfg.merge_from_file(args.config_file)
    # Merge configs from list generated by args.opts
    cfg.merge_from_list(args.opts)  # ['MODEL.DEVICE', 'cuda:0']
    cfg.MODEL.PRETRAIN = False
    cfg.MODEL.BACKBONE.PRETRAIN = False
    cfg.OUTPUT_DIR = os.path.join(cfg.OUTPUT_DIR, 'iterative_prune', f'sparsity_{args.sparsity}')
    cfg.freeze()
    default_setup(cfg, args, 'log_sparse_model_train.txt')
    return cfg


def retrain_sparse_model(cfg, args, model, train_loader):
    data_loader_iter = iter(train_loader)
    model.train()
    optimizer, _ = build_optimizer(cfg, model)
    iters_per_epoch = len(train_loader.dataset) // cfg.SOLVER.IMS_PER_BATCH
    scheduler = build_lr_scheduler(cfg, optimizer, iters_per_epoch)

    start_epoch = 0
    iteration = start_iter = start_epoch * iters_per_epoch
    max_epoch = cfg.SOLVER.MAX_EPOCH
    max_iter = max_epoch * iters_per_epoch
    warmup_iters = cfg.SOLVER.WARMUP_ITERS
    delay_epochs = cfg.SOLVER.DELAY_EPOCHS
    best_metric = -1
    # periodic_checkpointer = PeriodicCheckpointer(checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD, max_epoch)
    # periodic_checkpointer.get_sparsity(args.sparsity)
    if len(cfg.DATASETS.TESTS) == 1:
        metric_name = "metric"
    else:
        metric_name = cfg.DATASETS.TESTS[0] + "/metric"
    
    writers = (
        [
            CommonMetricPrinter(max_iter),
            # JSONWriter(os.path.join(cfg.OUTPUT_DIR, "metrics.json")),
            # TensorboardXWriter(cfg.OUTPUT_DIR)
        ]
        if comm.is_main_process()
        else []
    )

    with EventStorage(start_iter) as storage:
        for epoch in range(start_epoch, max_epoch):
            storage.epoch = epoch
            for _ in range(iters_per_epoch):
                data = next(data_loader_iter)
                storage.iter = iteration

                loss_dict = model(data)
                losses = sum(loss_dict.values())
                assert torch.isfinite(losses).all(), loss_dict

                loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}
                losses_reduced = sum(loss for loss in loss_dict_reduced.values())
                if comm.is_main_process():
                    storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)

                optimizer.zero_grad()
                losses.backward()
                optimizer.step()
                storage.put_scalar("lr", optimizer.param_groups[0]["lr"], smoothing_hint=False)

                # if isinstance(param_wrapper, ContiguousParams):
                #     param_wrapper.assert_buffer_is_valid()
                if iteration - start_iter > 5 and \
                        ((iteration + 1) % 200 == 0 or iteration == max_iter - 1) and \
                        ((iteration + 1) % iters_per_epoch != 0):
                    for writer in writers:
                        writer.write()
                
                iteration += 1

                if iteration <= warmup_iters:
                        scheduler["warmup_sched"].step()
            
            # Write metrics after each epoch
            for writer in writers:
                writer.write()

            if iteration > warmup_iters and (epoch + 1) > delay_epochs:
                    scheduler["lr_sched"].step()

            if (
                    cfg.TEST.EVAL_PERIOD > 0
                    and (epoch + 1) % cfg.TEST.EVAL_PERIOD == 0
                    and iteration != max_iter - 1
            ):
                results = do_test(cfg, model)
                # Compared to "train_net.py", the test results are not dumped to EventStorage
            else:
                results = {}
            flatten_results = flatten_results_dict(results)

            metric_dict = dict(metric=flatten_results[metric_name] if metric_name in flatten_results else -1)
            if metric_dict["metric"] > best_metric:
                print(f"current metric:{metric_dict['metric']}, best_metric:{best_metric}")
                best_metric = metric_dict["metric"]
                Checkpointer(model, cfg.OUTPUT_DIR).save(f"model_sparsity_{args.sparsity}")
                

def main(args):
    cfg = setup(args)
    train_loader = build_reid_train_loader(cfg)
    cfg = auto_scale_hyperparams(cfg, train_loader.dataset.num_classes)
    model = build_model(cfg)
    # Checkpointer(model).load(os.path.join(os.path.abs(cfg.OUTPUT_DIR), 'model_best.pth'))
    if args.last_sparsity == 0:
        model_path = os.path.abspath(os.path.join(cfg.OUTPUT_DIR, '../..', 'model_best.pth'))
    else:
        # cfg.OUTPUT_DIR: os.path.join(cfg.OUTPUT_DIR, 'pruning', f'sparsity_{args.sparsity}')
        model_path = os.path.join(os.path.dirname(cfg.OUTPUT_DIR), f"sparsity_{args.last_sparsity}/model_sparsity_{args.last_sparsity}.pth")
    Checkpointer(model).load(model_path)
    input_signature = torch.randn([1, 3, 256, 128], dtype=torch.float32)
    input_signature = input_signature.to(torch.device(cfg.MODEL.DEVICE))
    pruning_runner = get_pruning_runner(model, input_signature, 'iterative')
    model = pruning_runner.prune(removal_ratio=args.sparsity, mode='sparse',
                                 excludes=[model.backbone.layer4[2].conv3,
                                           model.heads])
    distributed = comm.get_world_size() > 1
    if distributed:
        model = DistributedDataParallel(model, device_ids=[comm.get_local_rank()], broadcast_buffers=False)
    retrain_sparse_model(cfg, args, model, train_loader)
    
    # return do_test(cfg, model)


if __name__ == "__main__":
    args = default_argument_parser().parse_args()
    print("Command Line Args:", args)
    launch(
        main,
        args.num_gpus,
        num_machines=args.num_machines,
        machine_rank=args.machine_rank,
        dist_url=args.dist_url,
        args=(args,),
    )